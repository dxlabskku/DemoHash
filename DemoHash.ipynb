{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "random.seed(1)\n",
    "import time\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {}\n",
    "with open(\"label.txt\") as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(' ')\n",
    "        tag_dict[int(key)] = val.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index = {v: k for k, v in tag_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "with open(\"vocab.txt\") as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split(' ')\n",
    "        vocab_dict[int(key)] = val.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {v: k for k, v in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### image feature\n",
    "h5 = h5py.File('insta_imgFeat__.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, Dense, Embedding, Dropout, LSTM, Lambda, Concatenate, \\\n",
    "    Multiply, RepeatVector, Permute, Flatten, Activation\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from selfDef import myLossFunc, Attention, coAttention_para, zero_padding, tagOffSet\n",
    "\n",
    "num_tags = len(tag_dict)\n",
    "num_words = len(vocab_dict)\n",
    "index_from_text = 3\n",
    "index_from_tag = 2\n",
    "seq_length = 328    # max length of text sequence\n",
    "batch_size = 5\n",
    "embedding_size = 300\n",
    "attention_size = 200\n",
    "dim_k = 100\n",
    "num_region = 7*7\n",
    "drop_rate = 0.75\n",
    "maxTagLen = 186    # max length of tag sequence\n",
    "num_epoch = 10\n",
    "numHist = 1    # historical posts number for each user\n",
    "numTestInst = 1280    # if you're going to use predict_generator, modify this parameter as your testSet size.\n",
    "testBatchSize = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig set : post_id, words, hashtags, user_id, post_num, follower, following, like_num, comment_num\n",
    "text_train, tag_train, ids_train, user_train, demo_train = pickle.load(open(\"trainSet2_20_%d.pkl\"%numHist, \"rb\"))\n",
    "text_train = zero_padding(text_train, seq_length)\n",
    "\n",
    "text_test, tag_test, ids_test, user_test, demo_test = pickle.load(open(\"testSet2_20_%d.pkl\"%numHist, \"rb\"))\n",
    "text_test = zero_padding(text_test, seq_length)\n",
    "tag_test = list(tag_test)\n",
    "tmp_test_tag = []\n",
    "for index in range(len(tag_test)-(numTestInst % testBatchSize)):\n",
    "    tmpArray = np.zeros(num_tags)\n",
    "    tmpArray[np.array(tag_test[index], dtype=np.int32)] = 1\n",
    "    tmp_test_tag.append(tmpArray)\n",
    "test_tag = np.array(tmp_test_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DemoHash model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchMaker(texts, tags, ids, users, demo):\n",
    "    shape = texts.shape[0]\n",
    "    text_copy = texts.copy()\n",
    "    tag_copy = tags.copy()\n",
    "    ids_copy = ids.copy()\n",
    "    users_copy = users.copy()\n",
    "    demo_copy = demo.copy()\n",
    "\n",
    "    indices = np.arange(shape)\n",
    "    np.random.shuffle(indices)\n",
    "    text_copy = list(text_copy[indices])\n",
    "    tag_copy = list(tag_copy[indices])\n",
    "    ids_copy = np.array(ids_copy)[indices]\n",
    "    users_copy = users_copy[indices]\n",
    "    demo_copy = demo_copy[indices]\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        if i + batch_size <= shape:\n",
    "            img_train = []\n",
    "            tmp_train_text = []\n",
    "            tmp_train_tag = []\n",
    "            tmp_train_demo = []\n",
    "\n",
    "            for index in range(i, i+batch_size):\n",
    "                data = h5.get(ids_copy[index])\n",
    "                np_data = np.array(data)\n",
    "                if np_data.shape != ():\n",
    "                    img_train.append(np_data)\n",
    "                    tmp_train_text.append(np.array(text_copy[index], dtype=np.int32))\n",
    "                    tmpArray = np.zeros(num_tags)\n",
    "                    tmpArray[np.array(tag_copy[index], dtype=np.int32)] = 1\n",
    "                    tmp_train_tag.append(tmpArray)\n",
    "                    tmp_train_demo.append(np.array(demo_copy[index], dtype=np.float32))\n",
    "            text_train = np.array(tmp_train_text)\n",
    "            tag_train = np.array(tmp_train_tag)\n",
    "            img_train = np.squeeze(np.array(img_train))\n",
    "            demo_train = np.array(tmp_train_demo)\n",
    "\n",
    "            yield [img_train, text_train, demo_train], tag_train\n",
    "            i+=batch_size\n",
    "        else:\n",
    "            i= 0\n",
    "            indices = np.arange(shape)\n",
    "            np.random.shuffle(indices)\n",
    "            text_copy = np.array(text_copy)\n",
    "            tag_copy = np.array(tag_copy)\n",
    "            text_copy = list(text_copy[indices])\n",
    "            tag_copy = list(tag_copy[indices])\n",
    "            ids_copy = np.array(ids_copy)[indices]\n",
    "            users_copy = users_copy[indices]\n",
    "            demo_copy = demo_copy[indices]\n",
    "            \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchMakerTest(texts, tags, ids, users, demo):\n",
    "    shape = texts.shape[0]\n",
    "    text_copy = texts.copy()\n",
    "    tag_copy = tags.copy()\n",
    "    ids_copy = ids.copy()\n",
    "    users_copy = users.copy()\n",
    "    demo_copy = demo.copy()\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i + testBatchSize <= shape:\n",
    "            img_test = []\n",
    "            tmp_test_text = []\n",
    "            tmp_test_tag = []\n",
    "            tmp_test_demo = []\n",
    "\n",
    "            for index in range(i, i + testBatchSize):\n",
    "                data = h5.get(ids_copy[index])\n",
    "                np_data = np.array(data)\n",
    "                if np_data.shape != ():\n",
    "                    img_test.append(np_data)                 \n",
    "                    tmp_test_text.append(np.array(text_copy[index], dtype=np.int32))\n",
    "                    tmpArray = np.zeros(num_tags)\n",
    "                    tmpArray[np.array(tag_copy[index], dtype=np.int32)] = 1\n",
    "                    tmp_test_tag.append(tmpArray)\n",
    "                    tmp_test_demo.append(np.array(demo_copy[index], dtype=np.float32))           \n",
    "            text_test = np.array(tmp_test_text)\n",
    "            tag_test = np.array(tmp_test_tag)\n",
    "            img_test = np.squeeze(np.array(img_test))\n",
    "            demo_test = np.array(tmp_test_demo)\n",
    "            yield [img_test, text_test, demo_test], tag_test\n",
    "            i += testBatchSize\n",
    "        else:\n",
    "            i = 0\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelDef():\n",
    "    inputs_img = Input(shape=(7, 7, 512))\n",
    "    inputs_text = Input(shape=(seq_length,))\n",
    "    inputs_demo = Input(shape=(133,))\n",
    "\n",
    "    # shared layers\n",
    "    tagEmbeddings = Embedding(input_dim=num_tags + index_from_tag, output_dim=embedding_size,\n",
    "                              mask_zero=True, input_length=maxTagLen)\n",
    "    textEmbeddings = Embedding(input_dim=num_words + index_from_text, output_dim=embedding_size,\n",
    "                               mask_zero=True, input_length=seq_length)\n",
    "    lstm = LSTM(units=embedding_size, return_sequences=True)\n",
    "    dense = Dense(embedding_size, activation=\"tanh\", use_bias=False)\n",
    "    reshape = Reshape(target_shape=(num_region, 512))\n",
    "    coAtt_layer = coAttention_para(dim_k=dim_k)\n",
    "    tag_att = Attention(attention_size)  \n",
    "    # Numerical Value \n",
    "    dense_numeric = Dense(32, activation = 'relu')\n",
    "\n",
    "    # query post representation\n",
    "    text_embeddings = textEmbeddings(inputs_text)\n",
    "    tFeature = lstm(text_embeddings)\n",
    "    iFeature = reshape(inputs_img)\n",
    "    iFeature = dense(iFeature)\n",
    "\n",
    "    demoFeature = Dense(embedding_size, activation='tanh')(inputs_demo)\n",
    "    \n",
    "    co_feature = coAtt_layer([tFeature, iFeature])\n",
    "    \n",
    "    # calculating similarity between demoFature and co_feature of the post\n",
    "    sim = Multiply()([demoFeature, co_feature])\n",
    "    att = Dense(1, activation='tanh')(sim)\n",
    "    att = Flatten()(att)\n",
    "    \n",
    "    # compute the weights of each demographic feature \n",
    "    attention = Activation('softmax')(att)\n",
    "    \n",
    "    attention = RepeatVector(embedding_size)(attention)\n",
    "    attention = Permute([2, 1])(attention)  \n",
    "    \n",
    "    # Multiply similarity and weights (weighted sum)\n",
    "    influence = Multiply()([sim, attention])\n",
    "    influence = Lambda(lambda x: K.sum(x, axis=1))(influence)\n",
    "    # the most important demographic feature of using hashtag\n",
    "    influence = Dense(embedding_size)(influence)\n",
    "    \n",
    "    h = Concatenate()([co_feature, influence])\n",
    "    dropout = Dropout(drop_rate)(h)\n",
    "    \n",
    "    Softmax = Dense(num_tags, activation=\"softmax\", use_bias=True)(dropout)\n",
    "    model = Model(inputs=[inputs_img, inputs_text, inputs_demo],\n",
    "                  outputs=[Softmax])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=myLossFunc)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(y_true, y_pred, top_K):\n",
    "    acc_count = 0\n",
    "    precision_K = []\n",
    "    recall_K = []\n",
    "    f1_K = []\n",
    "\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        top_indices = y_pred[i].argsort()[-top_K:]\n",
    "        if np.sum(y_true[i, top_indices]) >= 1:\n",
    "            acc_count += 1\n",
    "        p = np.sum(y_true[i, top_indices]) / top_K\n",
    "        r = np.sum(y_true[i, top_indices]) / np.sum(y_true[i, :])\n",
    "        precision_K.append(p)\n",
    "        recall_K.append(r)\n",
    "        if p != 0 or r != 0:\n",
    "            f1_K.append(2 * p * r / (p + r))\n",
    "        else:\n",
    "            f1_K.append(0)\n",
    "    acc_K = acc_count * 1.0 / y_pred.shape[0]\n",
    "\n",
    "    return acc_K, np.mean(np.array(precision_K)), np.mean(np.array(recall_K)), np.mean(np.array(f1_K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for top_K in [7]:\n",
    "        model = modelDef()\n",
    "\n",
    "        F = 0.0\n",
    "        res_file = open(\"record_userHist_%d_%d.txt\"%(embedding_size, numHist), \"a\")\n",
    "        string = \"Embedding_size = %d \\t Top- %d\\n\" % (embedding_size, top_K)\n",
    "        res_file.write(string)\n",
    "        print(\"Start Training...\")\n",
    "        start = time.time()\n",
    "        for epoch in range(num_epoch):\n",
    "            history = model.fit_generator(\n",
    "                generator=batchMaker(text_train, tag_train, ids_train, user_train, demo_train),\n",
    "                steps_per_epoch=int(text_train.shape[0] / batch_size),\n",
    "                epochs=1,\n",
    "                verbose=1,)\n",
    "            y_pred = model.predict_generator(generator=batchMakerTest(text_test, tag_test, ids_test, user_test, demo_test),\n",
    "                                             steps=int(numTestInst / testBatchSize),\n",
    "                                             verbose=1)\n",
    "            acc, precision, recall, f1 = evaluator(test_tag, y_pred, top_K)\n",
    "\n",
    "            print(\"Top %d, Epoch: %d,accuracy: %.6f, precision: %.6f, recall: %.6f, f1: %.6f\" %\n",
    "                  (top_K, epoch, acc, float(precision), float(recall), float(f1)))\n",
    "            if f1 >= F:\n",
    "                model.save_weights(\"model_best_%d_%d.h5\"%(embedding_size, numHist))\n",
    "                res_file = open(\"record_userHist_%d_%d.txt\"%(embedding_size, numHist), \"a\")\n",
    "                string = \"Epoch: %d,accuracy: %.6f, precision: %.6f, recall: %.6f, f1: %.6f \\n\" % (\n",
    "                epoch, acc, float(precision), float(recall), float(f1))\n",
    "                res_file.write(string)\n",
    "                res_file.close()\n",
    "                F = f1\n",
    "    print(\"time :\", time.time() - start)\n",
    "    print(\"Training Process Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_instagram_py36",
   "language": "python",
   "name": "gpu_instagram_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
